<p>```python {.marimo name=”setup”}</p>
<h1 id="initialization-code-that-runs-before-all-other-cells">Initialization code that runs before all other cells</h1>
<p>import marimo as mo</p>

<p>import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from helpers import configure_seaborn
import itertools
import warnings 
warnings.filterwarnings(“ignore”, category=RuntimeWarning)
configure_seaborn()
np.random.seed(308)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
#Reinforcement Learning Chapter 2: Multi-armed bandits
&lt;!----&gt;
## Introduction

This notebook explores the multi-armed bandit problem and ways to solve it using different algorithms.

The multi-armed bandit problem is a very nice introduction to more general RL problems. Some of the concepts that will be explored in this notebook include:
- Exploration / exploitation trade-off
- Action value estimates
- $\varepsilon$-greedy algorithm
- Optimistic initialization
- UCB action selection

We will implement several algorithms to solve the multi-armed bandit problem and compare their performance.

### Problem statement

The multi-armed bandit problem is defined as follows:

The agent is faced with $k$ arms. Pulling an arm returns a reward drawn from a probability distribution specific to that arm, but unknown to the agent. The agent chooses an action at each time step, and the goal is to maximize the total reward over time.

The reward obtained by pulling arm $k$ is drawn from a normal distribution $\mathcal{N}(\mu_k, \sigma^2)$, where $\mu_k$ is the true mean reward for arm $k$ and $\sigma^2$ is the variance. The variance is identical between all arms. The means $\mu_k$ are drawn from a normal distribution $\mathcal{N}(\mu, 1)$ at the start of each run, where $\mu$ is a hyperparameter of the environment.
&lt;!----&gt;
### Defining the environment

```python {.marimo}
class Arm:

    def __init__(self, mu, sigma=1):
        self.mu = mu
        self.sigma = sigma

    def pull(self, size=1):
        return np.random.normal(self.mu, self.sigma, size).squeeze()

class MultiArmedBandit:

    def __init__(self, k=10, mu=0, sigma=1):
        self.k = k
        self.mu = mu
        self.sigma = sigma
        self.arms = [Arm(np.random.normal(mu, 1), sigma) for _ in range(k)]
</code></pre></div></div>

<p>```python {.marimo}
bandit = MultiArmedBandit(k=10, mu=0, sigma=1)
rewards = np.array([arm.pull(size=10000) for arm in bandit.arms])</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
```python {.marimo hide_code="true"}
f, ax = plt.subplots(1, 1, figsize=(8, 5))
sns.violinplot(data=rewards.T, ax=ax)
plt.grid(True)
plt.title("Reward distributions of each arm")
plt.xlabel("Arm")
plt.ylabel("Reward")
plt.gca()
</code></pre></div></div>

<p>```python {.marimo hide_code=”true”}
mo.md(rf”””
From the above graph representing the distribution of rewards obtained by pulling each arm, we can clearly see that arm {np.argmax([arm.mu for arm in bandit.arms])} is the best choice.</p>

<p>However, the agent doesn’t know this: they must find the best arm through experience acquired by pulling the arms and observing the rewards.
“””)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## The $\varepsilon$-greedy algorithm

The $\varepsilon$-greedy algorithm is a simple algorithm that allows the agent to balance exploration and exploitation.

The algorithm works as follows:

1. Select an action:
    - With probability $\varepsilon$, select a random action **(exploration)**.
    -  With probability $1 - \varepsilon$, select the action with the highest estimated value:  $\arg \max_{a \in A} Q_t(a)$ **(exploitation)**.
2. Observe the reward $R_t(a)$
3. Update the estimated value of the selected action using the formula: $Q_{t+1}(a) = Q_t(a) + \frac{1}{N_a} (R_t - Q_t(a))$

```python {.marimo}
class Agent:

    def __init__(self, problem: MultiArmedBandit):
        self.problem = problem
        self.rewards = np.zeros(problem.k)
        self.action_counts = np.zeros(problem.k)
        self.q_values = np.zeros(problem.k)


class EpsilonGreedy(Agent):

    def __init__(self, problem: MultiArmedBandit, epsilon=0.1):
        super().__init__(problem)
        self.epsilon = epsilon

    def select_action(self):
        if np.random.rand() &lt; self.epsilon:
            return np.random.randint(self.problem.k)
        else:
            return np.argmax(self.q_values)

    def run(self, steps=1000):
        all_rewards = []
        for _ in range(steps):
            action = self.select_action()
            reward = self.problem.arms[action].pull()
            all_rewards.append(reward)
            self.action_counts[action] += 1
            self.rewards[action] += reward
            self.q_values[action] += (1 / self.action_counts[action]) * (
                reward - self.q_values[action]
            )
        return all_rewards
</code></pre></div></div>

<p>Now that the $\varepsilon$-greedy algorithm is implemented, we can run it on the multi-armed bandit problem and visualize the average reward over time.</p>

<p>We will use different values of $\varepsilon$ to see how it affects the performance of the algorithm.</p>

<p>```python {.marimo}
n_runs = 1000
steps = 3000
epsilon_values = [0.01, 0.1, 0.2]
total_rewards_epsilon = np.zeros((len(epsilon_values), n_runs, steps))
for _i, epsilon_value in enumerate(epsilon_values):
    for _n_run in range(n_runs):
        _learner = EpsilonGreedy(problem=bandit, epsilon=epsilon_value)
        _rew = _learner.run(steps=steps)
        total_rewards_epsilon[_i, _n_run] = np.array(_rew)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
```python {.marimo hide_code="true"}
mo.md(rf"""
Below, we plot the average reward over time, computed over {n_runs} runs for different $\varepsilon$ values.

We can clearly see the trade-off between exploration and exploitation in the graph below.

- A high value of $\varepsilon$ makes the agent **explore** faster, since suboptimal actions are chosen more often. The agent quickly discovers the optimal action, but continues to explore suboptimal actions, which lowers the average reward. Asymptotically, the optimal action will be selected with a probability of $1 - \varepsilon$.
- A low value of $\varepsilon$ makes the agent **exploit** more, since the optimal action is selected more often. However, the agent may take longer to discover the optimal action.
""")
</code></pre></div></div>

<p>```python {.marimo hide_code=”true”}
from helpers import running_avg</p>

<p><em>f, _axes = plt.subplots(1, 1, figsize=(8, 5))
for _i, epsilon_value</em> in enumerate(epsilon_values):
    <em>avg_rewards = total_rewards_epsilon[_i].mean(axis=0)
    _avg_rewards_mean = running_avg(_avg_rewards, window_size=10)
    _c = sns.color_palette()[_i]
    _axes.plot(_avg_rewards, alpha=0.4, color=_c)
    _axes.plot(_avg_rewards_mean, label=f”$\epsilon$ = {epsilon_value</em>}”, color=_c)
_axes.plot([bandit.arms[np.argmax([arm.mu for arm in bandit.arms])].mu]*steps, ‘k–’, label=”Optimal action value”, color=’black’, linewidth=0.5)
_axes.set_xlabel(“Steps”)
_axes.set_ylabel(“Average reward”)
plt.title(“Average reward evolution for different $\epsilon$ values”)
plt.legend()
plt.grid(True)
plt.gca()</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## Optimistic initialization

A way to encourage exploration is to initialize the action values $Q_0(a)$ to high values, higher than the true action values. At the beginning of the search, the agent is encouraged to explore all actions, since they all appear to be better than they actually are.

As the search progresses, the action values converge to their true values, and the agent exploits the best action.

*Note: optimistic initialization is extremely unsuited to non-stationary problems.*

```python {.marimo}
class OptimisticEpsilonGreedy(EpsilonGreedy):

    def __init__(self, problem: MultiArmedBandit, epsilon=0.1, initial_value=5.0):
        super().__init__(problem, epsilon)
        self.q_values = np.ones(problem.k) * initial_value
</code></pre></div></div>

<p>Let’s run the optimistic $\varepsilon$-greedy algorithm with different initial values and compare it to the standard $\varepsilon$-greedy algorithm.</p>

<p>```python {.marimo}
initial_values = [0.0, 0.5, 5.0]
steps_opt = 1000
total_rewards_optimistic = np.zeros((len(initial_values), n_runs, steps_opt))
for _i, initial_value in enumerate(initial_values):
    for _n_run in range(n_runs):
        _learner = OptimisticEpsilonGreedy(problem=bandit, epsilon=0.01, initial_value=initial_value)
        _rew = _learner.run(steps=steps_opt)
        total_rewards_optimistic[_i, _n_run] = np.array(_rew)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
```python {.marimo hide_code="true"}
mo.md(rf"""
Below, we plot the average reward over time, computed over {n_runs} runs for different $Q_0$ values.
We can see that optimistic initialization allows the agent to explore more at the beginning of the search, leading to a faster discovery of the optimal action.
""")
</code></pre></div></div>

<p>```python {.marimo hide_code=”true”}
<em>f, _axes = plt.subplots(1, 1, figsize=(8, 5))
for _i, initial_value</em> in enumerate(initial_values):
    <em>avg_rewards = total_rewards_optimistic[_i].mean(axis=0)
    _avg_rewards_mean = running_avg(_avg_rewards, window_size=10)
    _c = sns.color_palette()[_i]
    _axes.plot(_avg_rewards, alpha=0.4, color=_c)
    _axes.plot(_avg_rewards_mean, label=f”$Q_0$ = {initial_value</em>}”, color=_c)
_axes.plot([bandit.arms[np.argmax([arm.mu for arm in bandit.arms])].mu]*steps_opt, ‘k–’, label=”Optimal action value”, color=’black’, linewidth=0.5)
_axes.set_xlabel(“Steps”)
_axes.set_ylabel(“Average reward”)
plt.title(“Average reward evolution for different $Q_0$ values”)
plt.legend()
plt.grid(True)
plt.gca()</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## Upper Confidence Bound (UCB) action selection

The UCB algorithm is another way to balance exploration and exploitation. It selects actions based on both their estimated value and the uncertainty of that estimate.

Formally, the action selection is performed using the formula:
$A_t = \arg \max_{a \in A} \left( Q_t(a) + c \sqrt{\frac{\ln t}{N_a(t)}} \right)$

```python {.marimo}
class UCB(Agent):

    def __init__(self, problem: MultiArmedBandit, c=2):
        super().__init__(problem)
        self.c = c
        self.t = 0

    def select_action(self):
        self.t += 1
        ucb_values = self.q_values + self.c * np.sqrt(
            np.log(self.t) / (self.action_counts + 1e-5)
        )
        return np.argmax(ucb_values)

    def run(self, steps=1000):
        all_rewards = []
        for _ in range(steps):
            action = self.select_action()
            reward = self.problem.arms[action].pull()
            all_rewards.append(reward)
            self.action_counts[action] += 1
            self.rewards[action] += reward
            self.q_values[action] += (1 / self.action_counts[action]) * (
                reward - self.q_values[action]
            )
        return all_rewards
</code></pre></div></div>

<p>Let’s run the UCB algorithm.</p>

<p>```python {.marimo}
c_values = [0.01, 1.0, 3.0]
steps_ucb = 2000
bandit_ucb = MultiArmedBandit(k=10, mu=0, sigma=5)
total_rewards_ucb = np.zeros((len(c_values), n_runs, steps_ucb))
for _i, c_value in enumerate(c_values):
    for _n_run in range(n_runs):
        _learner = UCB(problem=bandit_ucb, c=c_value)
        _rew = _learner.run(steps=steps_ucb)
        total_rewards_ucb[_i, _n_run] = np.array(_rew)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
```python {.marimo hide_code="true"}
mo.md(rf"""
We plot the average reward over time, computed over {n_runs} runs for different $C$ values.

We can see the impact of the value of $C$ on the convergence of the algorithm. A lower value of $C$ explores less, leading to faster convergence but potentially suboptimal performance. A higher value of $C$ encourages more exploration, which can lead to better long-term performance but slower convergence.
""")
</code></pre></div></div>

<p>```python {.marimo}
<em>f, _axes = plt.subplots(1, 1, figsize=(8, 5))
for _i, c_value</em> in enumerate(c_values):
    <em>avg_rewards = total_rewards_ucb[_i].mean(axis=0)
    _avg_rewards_mean = running_avg(_avg_rewards, window_size=10)
    _c = sns.color_palette()[_i]
    _axes.plot(_avg_rewards, alpha=0.4, color=_c)
    _axes.plot(_avg_rewards_mean, label=f”$C$ = {c_value</em>}”, color=_c)
_axes.plot([bandit_ucb.arms[np.argmax([arm.mu for arm in bandit_ucb.arms])].mu]*steps_ucb, ‘k–’, label=”Optimal action value”, color=’black’, linewidth=0.5)
_axes.set_xlabel(“Steps”)
_axes.set_ylabel(“Average reward”)
plt.title(“Average reward evolution for different $C$ values”)
plt.legend()
plt.grid(True)
plt.gca()</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## Gradient bandits

Gradient bandits is another approach to the multi-armed bandit problem. Instead of estimating action values, the agent maintains a preference for each action and updates these preferences based on the received rewards.

This is useful for non-stationary problems, where the true action values change over time.

The update rule for gradient bandits is:

- For the selected action $A_t$:
  $H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(A_t))$
- For all other actions $a \neq A_t$:
  $H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a)$

Where:
- $\alpha$ is the step-size parameter
- $\bar{R}_t$ is the average reward up to time $t$
- $\pi_t(a)$ is the probability of selecting action $a$ at time $t$, computed using the softmax function on the preferences.

```python {.marimo}
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

class GradientBandit(Agent):

    def __init__(self, problem: MultiArmedBandit, alpha=0.1):
        super().__init__(problem)
        self.alpha = alpha
        self.preferences = np.zeros(problem.k)
        self.average_reward = 0
        self.t = 0

    def select_action(self):
        probabilities = softmax(self.preferences)
        return np.random.choice(np.arange(self.problem.k), p=probabilities)

    def run(self, steps=1000):
        all_rewards = []
        for _ in range(steps):
            action = self.select_action()
            reward = self.problem.arms[action].pull()
            all_rewards.append(reward)
            self.t += 1
            self.average_reward += (1 / self.t) * (reward - self.average_reward)
            for a in range(self.problem.k):
                if a == action:
                    self.preferences[a] += self.alpha * (reward - self.average_reward) * (1 - np.exp(self.preferences[a]) / np.sum(np.exp(self.preferences)))
                else:
                    self.preferences[a] -= self.alpha * (reward - self.average_reward) * (np.exp(self.preferences[a]) / np.sum(np.exp(self.preferences)))
        return all_rewards
</code></pre></div></div>

<p>Let’s run the gradient bandit algorithm with different alpha values.</p>

<p>```python {.marimo}
alpha_values = [0.1, 0.5, 1]
n_runs_gb = 100
steps_gb = 2000
total_rewards_gb = np.zeros((len(alpha_values), n_runs_gb, steps_gb))
for _i, alpha_value in enumerate(alpha_values):
    for _n_run in range(n_runs_gb):
        _learner = GradientBandit(problem=bandit, alpha=alpha_value)
        _rew = _learner.run(steps=steps_gb)
        total_rewards_gb[_i, _n_run] = np.array(_rew)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Below, we plot the average reward over time, computed over 100 runs for different $\alpha$ values.

The learning rate $\alpha$ controls how quickly the preferences are updated based on the received rewards.
- A higher $\alpha$ leads to faster learning but can also cause instability
- A lower $\alpha$ results in slower learning but more stable updates.

```python {.marimo hide_code="true"}
_f, _axes = plt.subplots(1, 1, figsize=(8, 5))
for _i, alpha_value_ in enumerate(alpha_values):
    _avg_rewards = total_rewards_gb[_i].mean(axis=0)
    _avg_rewards_mean = running_avg(_avg_rewards, window_size=10)
    _c = sns.color_palette()[_i]
    _axes.plot(_avg_rewards, alpha=0.4, color=_c)
    _axes.plot(_avg_rewards_mean, label=rf"$\alpha$ = {alpha_value_}", color=_c)
_axes.plot([bandit.arms[np.argmax([arm.mu for arm in bandit.arms])].mu]*steps_gb, 'k--', label="Optimal action value", color='black', linewidth=0.5)
_axes.set_xlabel("Steps")
_axes.set_ylabel("Average reward")
plt.title(r"Average reward evolution for different $\alpha$ values")
plt.legend()
plt.grid(True)
plt.gca()
</code></pre></div></div>

<p>##Conclusion</p>

<p>In this notebook, we explored the multi-armed bandit problem and implemented several algorithms to solve it, including the $\varepsilon$-greedy algorithm, optimistic initialization, UCB action selection, and gradient bandits. We analyzed how different hyperparameters affect the performance of each algorithm and visualized the average reward over time.</p>

<p>```python {.marimo}</p>

<p>```</p>
